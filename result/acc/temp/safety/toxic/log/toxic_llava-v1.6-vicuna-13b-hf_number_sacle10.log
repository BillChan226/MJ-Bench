2024-05-30 22:55:52 | INFO | alignment_rm_score | Loading reward model form llava-v1.6-vicuna-13b-hf...
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
2024-05-30 22:55:54 | INFO | accelerate.utils.modeling | We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:01<00:05,  1.14s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.11s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.08s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.06s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:05<00:01,  1.06s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:05<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:05<00:00,  1.03it/s]
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
2024-05-30 22:56:01 | INFO | alignment_rm_score | Loading prompt template from /cpfs01/user/duyichao/workspace/LLM_RLAIF/MM-Reward/eval/prompts/safety_single_number_scale10.txt
Evaluating RM: : 0it [00:00, ?it/s]Evaluating RM: : 1it [00:16, 16.40s/it]Evaluating RM: : 2it [00:26, 12.82s/it]Evaluating RM: : 3it [00:34, 10.40s/it]Evaluating RM: : 4it [00:42,  9.59s/it]2024-05-30 22:56:57 | INFO | alignment_rm_score | Cannot parsing the score from vlm output. sample id is 4
Evaluating RM: : 5it [00:55, 10.91s/it]Evaluating RM: : 6it [01:04, 10.29s/it]Evaluating RM: : 7it [01:12,  9.56s/it]Evaluating RM: : 8it [01:21,  9.36s/it]Evaluating RM: : 9it [01:30,  9.08s/it]Evaluating RM: : 10it [01:40,  9.25s/it]Evaluating RM: : 11it [01:51,  9.78s/it]Evaluating RM: : 12it [02:10, 12.73s/it]Evaluating RM: : 13it [02:21, 12.20s/it]Evaluating RM: : 14it [02:29, 11.01s/it]Evaluating RM: : 15it [02:40, 10.80s/it]Evaluating RM: : 16it [02:50, 10.78s/it]Evaluating RM: : 17it [03:00, 10.59s/it]Evaluating RM: : 18it [03:09,  9.89s/it]Evaluating RM: : 19it [03:17,  9.33s/it]Evaluating RM: : 20it [03:29, 10.23s/it]Evaluating RM: : 21it [03:36,  9.36s/it]Evaluating RM: : 22it [03:47,  9.61s/it]Evaluating RM: : 23it [03:56,  9.47s/it]2024-05-30 23:00:07 | INFO | alignment_rm_score | Cannot parsing the score from vlm output. sample id is 23
Evaluating RM: : 24it [04:05,  9.40s/it]Evaluating RM: : 25it [04:15,  9.56s/it]Evaluating RM: : 26it [04:25,  9.72s/it]Evaluating RM: : 27it [04:36, 10.06s/it]Evaluating RM: : 28it [04:46, 10.03s/it]Evaluating RM: : 29it [04:55,  9.75s/it]2024-05-30 23:01:06 | INFO | alignment_rm_score | Cannot parsing the score from vlm output. sample id is 29
Evaluating RM: : 30it [05:05,  9.81s/it]Evaluating RM: : 31it [05:15, 10.04s/it]Evaluating RM: : 32it [05:24,  9.70s/it]Evaluating RM: : 33it [05:33,  9.53s/it]Evaluating RM: : 34it [05:41,  8.87s/it]Evaluating RM: : 35it [05:50,  8.98s/it]Evaluating RM: : 36it [06:00,  9.40s/it]Evaluating RM: : 37it [06:09,  9.05s/it]Evaluating RM: : 38it [06:21, 10.12s/it]Evaluating RM: : 39it [06:33, 10.61s/it]Evaluating RM: : 40it [06:43, 10.42s/it]Evaluating RM: : 41it [06:54, 10.71s/it]Evaluating RM: : 42it [07:02,  9.67s/it]Evaluating RM: : 43it [07:11,  9.68s/it]Evaluating RM: : 44it [07:22,  9.94s/it]Evaluating RM: : 45it [07:33, 10.22s/it]Evaluating RM: : 46it [07:43, 10.17s/it]Evaluating RM: : 47it [07:52, 10.00s/it]Evaluating RM: : 48it [08:03, 10.13s/it]Evaluating RM: : 49it [08:14, 10.49s/it]Evaluating RM: : 50it [08:24, 10.32s/it]Evaluating RM: : 51it [08:32,  9.50s/it]Evaluating RM: : 52it [08:42,  9.82s/it]2024-05-30 23:04:59 | INFO | alignment_rm_score | Cannot parsing the score from vlm output. sample id is 52
Evaluating RM: : 53it [08:58, 11.51s/it]Evaluating RM: : 54it [09:06, 10.51s/it]Evaluating RM: : 55it [09:22, 12.23s/it]Evaluating RM: : 56it [09:33, 11.81s/it]Evaluating RM: : 57it [09:42, 11.05s/it]Evaluating RM: : 58it [09:51, 10.33s/it]Evaluating RM: : 59it [10:02, 10.59s/it]Evaluating RM: : 60it [10:14, 10.89s/it]Evaluating RM: : 61it [10:22, 10.07s/it]Evaluating RM: : 62it [10:33, 10.52s/it]Evaluating RM: : 63it [10:44, 10.58s/it]Evaluating RM: : 64it [10:57, 11.21s/it]Evaluating RM: : 65it [11:08, 11.19s/it]Evaluating RM: : 66it [11:17, 10.66s/it]Evaluating RM: : 67it [11:30, 11.19s/it]Evaluating RM: : 68it [11:42, 11.56s/it]Evaluating RM: : 69it [11:53, 11.36s/it]Evaluating RM: : 70it [12:01, 10.39s/it]Evaluating RM: : 71it [12:09,  9.60s/it]Evaluating RM: : 72it [12:22, 10.76s/it]Evaluating RM: : 73it [12:30,  9.86s/it]Evaluating RM: : 74it [12:38,  9.31s/it]Evaluating RM: : 75it [12:49,  9.70s/it]Evaluating RM: : 76it [12:57,  9.15s/it]Evaluating RM: : 77it [13:05,  8.76s/it]Evaluating RM: : 78it [13:18, 10.27s/it]Evaluating RM: : 79it [13:30, 10.57s/it]Evaluating RM: : 80it [13:39, 10.13s/it]Evaluating RM: : 81it [13:48,  9.77s/it]Evaluating RM: : 82it [13:58, 10.04s/it]Evaluating RM: : 83it [14:07,  9.59s/it]Evaluating RM: : 84it [14:17,  9.79s/it]Evaluating RM: : 85it [14:27,  9.80s/it]Evaluating RM: : 86it [14:37,  9.96s/it]Evaluating RM: : 87it [14:46,  9.61s/it]Evaluating RM: : 88it [14:55,  9.28s/it]Evaluating RM: : 89it [15:05,  9.75s/it]Evaluating RM: : 90it [15:17, 10.20s/it]Evaluating RM: : 90it [15:17, 10.19s/it]
2024-05-30 23:11:18 | INFO | alignment_rm_score | Saving result to /cpfs01/user/duyichao/workspace/LLM_RLAIF/MM-Reward/eval/results/safety/toxic
