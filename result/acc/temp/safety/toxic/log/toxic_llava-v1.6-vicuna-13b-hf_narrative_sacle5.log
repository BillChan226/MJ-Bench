2024-05-30 22:57:21 | INFO | alignment_rm_score | Loading reward model form llava-v1.6-vicuna-13b-hf...
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
2024-05-30 22:57:24 | INFO | accelerate.utils.modeling | We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:01<00:05,  1.07s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.02s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.00s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.02s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:05<00:01,  1.02s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:05<00:00,  1.20it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:05<00:00,  1.08it/s]
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
2024-05-30 22:57:31 | INFO | alignment_rm_score | Loading prompt template from /cpfs01/user/duyichao/workspace/LLM_RLAIF/MM-Reward/eval/prompts/safety_single_narrative_scale5.txt
Evaluating RM: : 0it [00:00, ?it/s]Evaluating RM: : 1it [00:13, 13.19s/it]Evaluating RM: : 2it [00:24, 12.35s/it]Evaluating RM: : 3it [00:31,  9.90s/it]Evaluating RM: : 4it [00:41,  9.60s/it]Evaluating RM: : 5it [00:52, 10.36s/it]Evaluating RM: : 6it [01:02, 10.04s/it]Evaluating RM: : 7it [01:10,  9.56s/it]Evaluating RM: : 8it [01:20,  9.59s/it]Evaluating RM: : 9it [01:29,  9.42s/it]2024-05-30 22:59:10 | INFO | alignment_rm_score | Cannot parsing the score from vlm output. sample id is 9
Evaluating RM: : 10it [01:39,  9.59s/it]Evaluating RM: : 11it [01:49,  9.63s/it]Evaluating RM: : 12it [02:08, 12.44s/it]Evaluating RM: : 13it [02:18, 11.76s/it]Evaluating RM: : 14it [02:27, 11.15s/it]Evaluating RM: : 15it [02:37, 10.64s/it]Evaluating RM: : 16it [02:52, 12.00s/it]Evaluating RM: : 17it [03:00, 10.88s/it]Evaluating RM: : 18it [03:11, 10.92s/it]Evaluating RM: : 19it [03:19, 10.06s/it]Evaluating RM: : 20it [03:33, 10.98s/it]Evaluating RM: : 21it [03:40,  9.91s/it]Evaluating RM: : 22it [03:50,  9.91s/it]Evaluating RM: : 23it [04:02, 10.43s/it]Evaluating RM: : 24it [04:09,  9.56s/it]2024-05-30 23:01:50 | INFO | alignment_rm_score | Cannot parsing the score from vlm output. sample id is 24
Evaluating RM: : 25it [04:19,  9.59s/it]Evaluating RM: : 26it [04:28,  9.36s/it]Evaluating RM: : 27it [04:38,  9.72s/it]Evaluating RM: : 28it [04:50, 10.41s/it]Evaluating RM: : 29it [05:00, 10.38s/it]2024-05-30 23:02:41 | INFO | alignment_rm_score | Cannot parsing the score from vlm output. sample id is 29
Evaluating RM: : 30it [05:10, 10.11s/it]Evaluating RM: : 31it [05:25, 11.47s/it]Evaluating RM: : 32it [05:35, 11.26s/it]Evaluating RM: : 33it [05:44, 10.51s/it]Evaluating RM: : 34it [05:52,  9.67s/it]Evaluating RM: : 35it [06:01,  9.64s/it]Evaluating RM: : 36it [06:17, 11.35s/it]Evaluating RM: : 37it [06:26, 10.70s/it]Evaluating RM: : 38it [06:38, 11.22s/it]Evaluating RM: : 39it [06:49, 11.08s/it]Evaluating RM: : 40it [06:59, 10.85s/it]Evaluating RM: : 41it [07:10, 10.79s/it]Evaluating RM: : 42it [07:18, 10.04s/it]Evaluating RM: : 43it [07:29, 10.27s/it]Evaluating RM: : 44it [07:39, 10.23s/it]Evaluating RM: : 45it [07:49, 10.11s/it]Evaluating RM: : 46it [07:57,  9.58s/it]Evaluating RM: : 47it [08:06,  9.37s/it]Evaluating RM: : 48it [08:17,  9.84s/it]Evaluating RM: : 49it [08:29, 10.43s/it]Evaluating RM: : 50it [08:40, 10.54s/it]Evaluating RM: : 51it [08:48,  9.68s/it]Evaluating RM: : 52it [08:58,  9.92s/it]Evaluating RM: : 53it [09:09, 10.36s/it]Evaluating RM: : 54it [09:17,  9.61s/it]Evaluating RM: : 55it [09:31, 10.85s/it]2024-05-30 23:07:13 | INFO | alignment_rm_score | Cannot parsing the score from vlm output. sample id is 55
Evaluating RM: : 56it [09:42, 10.76s/it]Evaluating RM: : 57it [09:49,  9.87s/it]Evaluating RM: : 58it [09:58,  9.36s/it]Evaluating RM: : 59it [10:08,  9.84s/it]Evaluating RM: : 60it [10:21, 10.71s/it]Evaluating RM: : 61it [10:29,  9.94s/it]Evaluating RM: : 62it [10:40, 10.12s/it]Evaluating RM: : 63it [10:52, 10.77s/it]Evaluating RM: : 64it [11:10, 12.73s/it]Evaluating RM: : 65it [11:18, 11.50s/it]Evaluating RM: : 66it [11:28, 10.94s/it]Evaluating RM: : 67it [11:38, 10.64s/it]Evaluating RM: : 68it [11:51, 11.51s/it]Evaluating RM: : 69it [12:01, 11.11s/it]Evaluating RM: : 70it [12:10, 10.23s/it]Evaluating RM: : 71it [12:19, 10.07s/it]Evaluating RM: : 72it [12:40, 13.15s/it]Evaluating RM: : 73it [12:48, 11.65s/it]Evaluating RM: : 74it [12:57, 10.79s/it]Evaluating RM: : 75it [13:06, 10.31s/it]Evaluating RM: : 76it [13:14,  9.58s/it]Evaluating RM: : 77it [13:23,  9.62s/it]Evaluating RM: : 78it [13:33,  9.73s/it]Evaluating RM: : 79it [13:43,  9.83s/it]Evaluating RM: : 80it [13:53,  9.62s/it]Evaluating RM: : 81it [14:04, 10.15s/it]Evaluating RM: : 82it [14:18, 11.30s/it]Evaluating RM: : 83it [14:26, 10.41s/it]Evaluating RM: : 84it [14:37, 10.49s/it]Evaluating RM: : 85it [14:47, 10.52s/it]Evaluating RM: : 86it [14:57, 10.25s/it]Evaluating RM: : 87it [15:07, 10.03s/it]Evaluating RM: : 88it [15:15,  9.46s/it]Evaluating RM: : 89it [15:31, 11.35s/it]Evaluating RM: : 90it [15:42, 11.36s/it]Evaluating RM: : 90it [15:42, 10.47s/it]
2024-05-30 23:13:13 | INFO | alignment_rm_score | Saving result to /cpfs01/user/duyichao/workspace/LLM_RLAIF/MM-Reward/eval/results/safety/toxic
