2024-05-30 21:40:11 | INFO | alignment_rm_score | Loading reward model form llava-v1.6-vicuna-13b-hf...
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
2024-05-30 21:40:15 | INFO | accelerate.utils.modeling | We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.02it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:01<00:03,  1.05it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:02<00:02,  1.09it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:03<00:01,  1.06it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:04<00:00,  1.07it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:05<00:00,  1.28it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:05<00:00,  1.16it/s]
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
2024-05-30 21:40:22 | INFO | alignment_rm_score | Loading prompt template from /cpfs01/user/duyichao/workspace/LLM_RLAIF/MM-Reward/eval/prompts/alignment_single_narrative_scale10.txt
Evaluating RM: : 0it [00:00, ?it/s]Evaluating RM: : 1it [00:12, 12.28s/it]Evaluating RM: : 2it [00:30, 15.55s/it]Evaluating RM: : 3it [00:46, 15.98s/it]Evaluating RM: : 4it [01:13, 20.22s/it]Evaluating RM: : 5it [01:24, 17.05s/it]Evaluating RM: : 6it [01:37, 15.63s/it]Evaluating RM: : 7it [01:50, 14.67s/it]Evaluating RM: : 8it [02:03, 14.13s/it]Evaluating RM: : 9it [02:20, 15.05s/it]Evaluating RM: : 10it [02:38, 16.02s/it]Evaluating RM: : 11it [02:56, 16.50s/it]Evaluating RM: : 12it [03:12, 16.54s/it]Evaluating RM: : 13it [03:23, 14.82s/it]Evaluating RM: : 14it [03:42, 15.91s/it]Evaluating RM: : 15it [03:51, 13.90s/it]Evaluating RM: : 16it [04:01, 12.77s/it]Evaluating RM: : 17it [04:12, 12.33s/it]Evaluating RM: : 18it [04:25, 12.36s/it]Evaluating RM: : 19it [04:35, 11.82s/it]Evaluating RM: : 20it [04:50, 12.61s/it]Evaluating RM: : 21it [05:01, 12.24s/it]Evaluating RM: : 22it [05:17, 13.44s/it]Evaluating RM: : 23it [05:28, 12.52s/it]Evaluating RM: : 24it [05:41, 12.85s/it]Evaluating RM: : 25it [06:07, 16.65s/it]2024-05-30 21:46:43 | INFO | alignment_rm_score | Cannot parsing the score from vlm output. sample id is 25
Evaluating RM: : 26it [06:20, 15.74s/it]2024-05-30 21:47:08 | INFO | alignment_rm_score | Cannot parsing the score from vlm output. sample id is 26
Evaluating RM: : 27it [06:46, 18.67s/it]Evaluating RM: : 28it [07:04, 18.49s/it]2024-05-30 21:47:45 | INFO | alignment_rm_score | Cannot parsing the score from vlm output. sample id is 28
Evaluating RM: : 29it [07:22, 18.48s/it]Evaluating RM: : 30it [07:32, 15.91s/it]Evaluating RM: : 31it [07:42, 13.90s/it]Evaluating RM: : 32it [07:52, 12.89s/it]Evaluating RM: : 33it [08:01, 11.69s/it]Evaluating RM: : 34it [08:24, 15.09s/it]Evaluating RM: : 35it [08:37, 14.39s/it]Evaluating RM: : 36it [08:47, 13.19s/it]Evaluating RM: : 37it [09:05, 14.71s/it]Evaluating RM: : 38it [09:17, 13.69s/it]Evaluating RM: : 39it [09:26, 12.37s/it]Evaluating RM: : 40it [09:36, 11.78s/it]Evaluating RM: : 41it [09:45, 10.74s/it]Evaluating RM: : 42it [09:57, 11.17s/it]Evaluating RM: : 43it [10:06, 10.53s/it]Evaluating RM: : 44it [10:18, 11.10s/it]Evaluating RM: : 45it [10:31, 11.49s/it]Evaluating RM: : 46it [10:41, 11.19s/it]Evaluating RM: : 47it [11:00, 13.39s/it]Evaluating RM: : 48it [11:16, 14.15s/it]Evaluating RM: : 49it [11:24, 12.49s/it]Evaluating RM: : 50it [11:37, 12.57s/it]Evaluating RM: : 51it [11:49, 12.49s/it]Evaluating RM: : 52it [12:06, 13.63s/it]2024-05-30 21:52:55 | INFO | alignment_rm_score | Cannot parsing the score from vlm output. sample id is 52
Evaluating RM: : 53it [12:33, 17.69s/it]Evaluating RM: : 54it [12:45, 15.91s/it]Evaluating RM: : 55it [12:50, 12.76s/it]Evaluating RM: : 56it [13:02, 12.64s/it]Evaluating RM: : 56it [13:02, 13.98s/it]
2024-05-30 21:53:25 | INFO | alignment_rm_score | Saving result to /cpfs01/user/duyichao/workspace/LLM_RLAIF/MM-Reward/eval/results/alignment/counts
