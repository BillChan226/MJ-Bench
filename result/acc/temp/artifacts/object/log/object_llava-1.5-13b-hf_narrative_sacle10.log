2024-05-30 21:36:44 | INFO | artifacte_rm_score | Loading reward model form llava-1.5-13b-hf...
/opt/conda/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:100: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:03,  1.49it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:01<00:02,  1.73it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:01<00:01,  1.81it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:02<00:01,  1.80it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:02<00:00,  1.83it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:03<00:00,  2.21it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:03<00:00,  1.96it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-05-30 21:36:56 | INFO | artifacte_rm_score | Loading dataset from /cpfs01/user/duyichao/workspace/LLM_RLAIF/MM-Reward/artifacts/objects_indoor...
2024-05-30 21:36:56 | INFO | artifacte_rm_score | Loading prompt template from /cpfs01/user/duyichao/workspace/LLM_RLAIF/MM-Reward/eval/prompts/artifacts_single_narrative_scale10.txt
Evaluating RM: : 0it [00:00, ?it/s]Evaluating RM: : 1it [00:16, 16.19s/it]Evaluating RM: : 2it [00:27, 13.54s/it]Evaluating RM: : 3it [00:39, 12.70s/it]Evaluating RM: : 4it [00:51, 12.23s/it]Evaluating RM: : 5it [01:06, 13.30s/it]Evaluating RM: : 6it [01:19, 13.34s/it]Evaluating RM: : 7it [01:33, 13.46s/it]Evaluating RM: : 8it [01:48, 13.94s/it]Evaluating RM: : 9it [02:01, 13.55s/it]Evaluating RM: : 10it [02:16, 14.27s/it]Evaluating RM: : 11it [02:30, 14.10s/it]Evaluating RM: : 12it [02:43, 13.74s/it]Evaluating RM: : 13it [02:56, 13.45s/it]Evaluating RM: : 14it [03:09, 13.36s/it]Evaluating RM: : 15it [03:23, 13.65s/it]Evaluating RM: : 16it [03:39, 14.25s/it]Evaluating RM: : 17it [03:51, 13.67s/it]Evaluating RM: : 18it [04:03, 13.15s/it]Evaluating RM: : 19it [04:16, 12.89s/it]Evaluating RM: : 20it [04:31, 13.53s/it]Evaluating RM: : 21it [04:42, 12.98s/it]Evaluating RM: : 22it [04:56, 13.09s/it]Evaluating RM: : 23it [05:08, 12.93s/it]Evaluating RM: : 24it [05:23, 13.52s/it]Evaluating RM: : 25it [05:35, 12.98s/it]Evaluating RM: : 26it [05:50, 13.52s/it]Evaluating RM: : 27it [06:03, 13.64s/it]Evaluating RM: : 28it [06:18, 13.91s/it]Evaluating RM: : 29it [06:31, 13.59s/it]Evaluating RM: : 30it [06:43, 13.26s/it]Evaluating RM: : 31it [06:57, 13.35s/it]Evaluating RM: : 32it [07:09, 13.02s/it]Evaluating RM: : 33it [07:24, 13.44s/it]Evaluating RM: : 34it [07:37, 13.43s/it]Evaluating RM: : 35it [07:51, 13.68s/it]Evaluating RM: : 36it [08:04, 13.40s/it]Evaluating RM: : 37it [08:19, 13.83s/it]Evaluating RM: : 38it [08:31, 13.38s/it]Evaluating RM: : 39it [08:44, 13.10s/it]Evaluating RM: : 40it [08:56, 12.81s/it]Evaluating RM: : 41it [09:08, 12.62s/it]Evaluating RM: : 42it [09:21, 12.74s/it]Evaluating RM: : 43it [09:36, 13.49s/it]Evaluating RM: : 44it [09:50, 13.53s/it]Evaluating RM: : 45it [10:02, 13.25s/it]Evaluating RM: : 46it [10:17, 13.74s/it]Evaluating RM: : 47it [10:30, 13.52s/it]Evaluating RM: : 48it [10:44, 13.59s/it]Evaluating RM: : 49it [10:58, 13.66s/it]Evaluating RM: : 50it [11:12, 13.75s/it]Evaluating RM: : 51it [11:25, 13.45s/it]Evaluating RM: : 52it [11:39, 13.82s/it]Evaluating RM: : 53it [11:52, 13.57s/it]Evaluating RM: : 54it [12:07, 13.94s/it]Evaluating RM: : 55it [12:21, 13.81s/it]Evaluating RM: : 56it [12:34, 13.68s/it]Evaluating RM: : 57it [12:49, 14.23s/it]Evaluating RM: : 58it [13:02, 13.71s/it]Evaluating RM: : 59it [13:17, 14.15s/it]Evaluating RM: : 60it [13:32, 14.31s/it]Evaluating RM: : 61it [13:45, 13.87s/it]Evaluating RM: : 62it [13:59, 14.14s/it]Evaluating RM: : 63it [14:12, 13.54s/it]Evaluating RM: : 64it [14:22, 12.55s/it]Evaluating RM: : 65it [14:35, 12.65s/it]Evaluating RM: : 66it [14:49, 13.21s/it]Evaluating RM: : 67it [15:02, 12.96s/it]Evaluating RM: : 68it [15:15, 12.99s/it]Evaluating RM: : 69it [15:30, 13.77s/it]Evaluating RM: : 70it [15:45, 14.12s/it]Evaluating RM: : 71it [15:59, 13.88s/it]Evaluating RM: : 72it [16:10, 13.05s/it]Evaluating RM: : 73it [16:24, 13.49s/it]Evaluating RM: : 74it [16:37, 13.44s/it]Evaluating RM: : 75it [16:50, 13.24s/it]Evaluating RM: : 76it [17:03, 13.18s/it]Evaluating RM: : 77it [17:15, 12.72s/it]Evaluating RM: : 78it [17:26, 12.23s/it]Evaluating RM: : 79it [17:40, 12.72s/it]Evaluating RM: : 80it [17:53, 12.85s/it]Evaluating RM: : 81it [18:06, 12.88s/it]Evaluating RM: : 82it [18:19, 13.01s/it]Evaluating RM: : 83it [18:33, 13.10s/it]Evaluating RM: : 84it [18:46, 13.21s/it]Evaluating RM: : 85it [19:01, 13.66s/it]Evaluating RM: : 86it [19:14, 13.41s/it]Evaluating RM: : 87it [19:27, 13.42s/it]Evaluating RM: : 88it [19:43, 14.18s/it]Evaluating RM: : 89it [19:57, 14.01s/it]Evaluating RM: : 90it [20:11, 14.22s/it]Evaluating RM: : 91it [20:25, 13.98s/it]Evaluating RM: : 92it [20:39, 13.92s/it]Evaluating RM: : 93it [20:51, 13.45s/it]Evaluating RM: : 94it [21:05, 13.77s/it]Evaluating RM: : 95it [21:17, 13.24s/it]Evaluating RM: : 96it [21:31, 13.22s/it]Evaluating RM: : 97it [21:43, 13.06s/it]Evaluating RM: : 98it [21:54, 12.40s/it]Evaluating RM: : 99it [22:06, 12.34s/it]Evaluating RM: : 100it [22:20, 12.88s/it]Evaluating RM: : 100it [22:20, 13.41s/it]
2024-05-30 21:59:17 | INFO | artifacte_rm_score | Saving result to /cpfs01/user/duyichao/workspace/LLM_RLAIF/MM-Reward/eval/results/artifacts/object
